# -*- coding: utf-8 -*-
"""Stock Market Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZIhWmzKrcbuvqOl366gCNWpF9M5P2e0d
"""

#Numpy is used for array computations in a very short time
import numpy as np

#Pandas is used to load the data frame in a 2D array
#has many functions for data analysis
import pandas as pd

#used to draw visualizations
import matplotlib.pyplot as plt
import seaborn as sb
import plotly.graph_objects as go

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

#Mounting the drive
#importing the dataset
#microsoft_stock_price_data = mspd
file_path = '/content/drive/My Drive/MSFT.csv'
mspd = pd.read_csv(file_path, date_parser= True)
mspd.head()

#converts the 'Date' column of the DataFrame mspd into a datetime object
# it allows us to manipulate the dates more easily, perform time-series analysis, and enables better formatting and handling of dates
mspd['Date'] = pd.to_datetime(mspd['Date'])
#sets the 'Date' column as the index of the DataFrame mspd
mspd.set_index('Date', inplace=True)
mspd.head(10)

#shape of the dataset
mspd.shape

#description of data
mspd.describe()

#Visualization of the data
plt.figure(figsize=(10,5))
plt.plot(mspd['Close'])
plt.title('Microsoft Close Price', fontsize=10)
plt.ylabel('Price in dollars.')
plt.show()

#Data Preprocessing

# Fill any missing values with the last valid observed value without creating a new dataframe using inplace = True
mspd.fillna(method='ffill', inplace=True)

# Add technical indicators as features
#Simple Moving Average
#Exponential Moving Average
mspd['SMA_20'] = mspd['Close'].rolling(window=20).mean()
mspd['EMA_20'] = mspd['Close'].ewm(span=20, adjust=False).mean()


#comparing today's close price with the next day close price
# Prepare a binary target for Logistic Regression
mspd['Target'] = np.where(mspd['Close'].shift(-1) > mspd['Close'], 1, 0)

# Dropping rows with NaN values
mspd.dropna(inplace=True)

mspd.head(10)

#Exploratory Data Analysis

plt.figure(figsize=(14, 7))
plt.plot(mspd['Close'], label='Close Price')
plt.plot(mspd['SMA_20'], label='20-day SMA')
plt.plot(mspd['EMA_20'], label='20-day EMA')
plt.title('Stock Price with Moving Averages')
plt.legend()
plt.show()

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Split data into features and target
features = mspd[['Close', 'SMA_20', 'EMA_20']]
print(features)

#filling the close price with missing values
y_regression = mspd['Close'].shift(-1).fillna(method='ffill')  # Target for regression models
y_classification = mspd['Target']  # Target for classification models

# Scaling the features
scaler = MinMaxScaler() #(x-x.min)/(x.max-x.min)
x_scaled = scaler.fit_transform(features)

# Split data for regression
x_train_regression, x_test_regression, y_train_regression, y_test_regression = train_test_split(x_scaled, y_regression, test_size=0.1, random_state=42)

# Split data for classification
x_train_classification, x_test_classification, y_train_classification, y_test_classification = train_test_split(x_scaled, y_classification, test_size=0.1, random_state=42)

"""**Model Training and Prediction**"""

#Linear regression

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, classification_report, accuracy_score
'''
lr_model = LinearRegression()
lr_model.fit(X_train_reg, y_train_reg)
lr_predictions = lr_model.predict(X_test_reg)
print(f"Linear Regression RMSE: {np.sqrt(mean_squared_error(y_test_reg, lr_predictions))}")
'''

import numpy as np

#Prepare the data by adding a column of ones to X_train_regression and X_test_regression to account for the intercept

x_train_regression_intercept = np.c_[np.ones((x_train_regression.shape[0], 1)), x_train_regression]
x_test_regression_intercept = np.c_[np.ones((x_test_regression.shape[0], 1)), x_test_regression]

#Calculate the optimal weights using the Normal Equation
# theta = (X^T * X)^(-1) * X^T * y
theta = np.linalg.inv(x_train_regression_intercept.T @ x_train_regression_intercept) @ x_train_regression_intercept.T @ y_train_regression

#Predict the outcomes on the testing set
lr_predictions = x_test_regression_intercept @ theta

#Calculation of the RMSE Value for the predictions
rmse = np.sqrt(mean_squared_error(y_test_regression, lr_predictions))
print(f"Linear Regression RMSE: {rmse}")

from sklearn.metrics import mean_squared_error, classification_report, accuracy_score

#Logistic Regression
'''
from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(X_train_class, y_train_class)
log_predictions = log_model.predict(X_test_class)
print(f"Logistic Regression Accuracy: {accuracy_score(y_test_class, log_predictions)}")
print(classification_report(y_test_class, log_predictions))
'''

import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def predict(x, weights):
    z = np.dot(x, weights)
    return sigmoid(z)

def logistic_regression(x, y, num_steps, learning_rate, add_intercept=False):
    if add_intercept:
        intercept = np.ones((x.shape[0], 1))
        x = np.hstack((intercept, x))

    # Initialize weights
    weights = np.zeros(x.shape[1])

    # Gradient descent
    for step in range(num_steps):
        scores = np.dot(x, weights)
        predictions = sigmoid(scores)

        # Update weights with gradient
        output_error_signal = y - predictions
        gradient = np.dot(x.T, output_error_signal)
        weights += learning_rate * gradient

        # Print log-likelihood every so often
        if step % 10000 == 0:
            print(f'Log-Likelihood at step {step}: {np.sum(-y * np.log(predictions) - (1 - y) * np.log(1 - predictions))}')

    return weights

# Train logistic regression model
weights = logistic_regression(x_train_classification, y_train_classification, num_steps=300000, learning_rate=0.001, add_intercept=True)

# Predict on test set
X_test_class_with_intercept = np.hstack((np.ones((x_test_classification.shape[0], 1)), x_test_classification))
final_scores = predict(X_test_class_with_intercept, weights)
predictions = np.round(final_scores)

# Evaluate the model
accuracy = np.mean(predictions == y_test_classification)
print(f"Logistic Regression Accuracy: {accuracy}")

from sklearn.metrics import classification_report
print(classification_report(y_test_classification, predictions))

#Support Vector Machine

'''
from sklearn.svm import SVR

svm_model = SVR()
svm_model.fit(x_train_regression, y_train_regression)
svm_predictions = svm_model.predict(x_test_regression)
print(f"SVM Regression RMSE: {np.sqrt(mean_squared_error(y_test_regression, svm_predictions))}")
'''

import numpy as np

from sklearn.metrics import mean_squared_error

class SVR:
    def __init__(self, epsilon=0.1, C=1.0, learning_rate=0.01, num_iterations=1000):
        self.epsilon = epsilon
        self.C = C
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.weights = None
        self.bias = 0

    def fit(self, x, y):
        num_samples, num_features = x.shape
        self.weights = np.zeros(num_features)
        for _ in range(self.num_iterations):
            for i in range(num_samples):
                condition = abs(np.dot(x[i], self.weights) + self.bias - y[i]) > self.epsilon
                if condition:
                    # Update weights and bias using gradient descent
                    gradient_weights = self.C * (y[i] - np.dot(x[i], self.weights) - self.bias) * x[i]
                    gradient_bias = self.C * (y[i] - np.dot(x[i], self.weights) - self.bias)
                    self.weights += self.learning_rate * gradient_weights
                    self.bias += self.learning_rate * gradient_bias
                else:
                    # Apply regularization term (penalizing the weights)
                    self.weights *= (1 - self.learning_rate)

    def predict(self, x):
        return np.dot(x, self.weights) + self.bias


svm_model = SVR(epsilon=0.1, C=1.0, learning_rate=0.01, num_iterations=1000)
svm_model.fit(x_train_regression, y_train_regression)
svm_predictions = svm_model.predict(x_test_regression)

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test_regression, svm_predictions))
print(f"SVM Regression RMSE: {rmse}")

# Decision Tree for Regression

'''
from sklearn.tree import DecisionTreeRegressor

dt_model = DecisionTreeRegressor()
dt_model.fit(X_train_reg, y_train_reg)
dt_predictions = dt_model.predict(X_test_reg)
print(f"Decision Tree Regression RMSE: {np.sqrt(mean_squared_error(y_test_reg, dt_predictions))}")
'''

import numpy as np
from sklearn.metrics import mean_squared_error

class DecisionTreeRegressor:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = {}

    def fit(self, X, y, depth=0):
        if len(X) == 0:
            return None
        if self.max_depth is not None and depth >= self.max_depth:
            return np.mean(y)  # Return mean of target values at max depth

        # Select the best split
        best_split = self._best_split(X, y)
        if best_split['variance'] == 0:
            return np.mean(y)  # Return mean if no further reduction in variance

        # Recursively build left and right branches
        left_indices = X[:, best_split['feature_index']] < best_split['threshold']
        right_indices = X[:, best_split['feature_index']] >= best_split['threshold']
        left_subtree = self.fit(X[left_indices], y[left_indices], depth + 1)
        right_subtree = self.fit(X[right_indices], y[right_indices], depth + 1)

        # Node structure
        node = {
            "feature_index": best_split['feature_index'],
            "threshold": best_split['threshold'],
            "left": left_subtree,
            "right": right_subtree,
            "variance": best_split['variance']
        }
        if depth == 0:
            self.tree = node
        return node

    def predict(self, X):
        return np.array([self._predict_one(x, self.tree) for x in X])

    def _predict_one(self, x, tree):
        if tree is None or isinstance(tree, float):
            return tree
        if x[tree["feature_index"]] < tree["threshold"]:
            return self._predict_one(x, tree["left"])
        else:
            return self._predict_one(x, tree["right"])

    def _best_split(self, X, y):
        best_variance = float("inf")
        best_feature_index = 0
        best_threshold = X[0, 0]
        for feature_index in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_index])
            for threshold in thresholds:
                left_y = y[X[:, feature_index] < threshold]
                right_y = y[X[:, feature_index] >= threshold]
                if len(left_y) == 0 or len(right_y) == 0:
                    continue
                variance = self._weighted_variance(left_y, right_y)
                if variance < best_variance:
                    best_variance = variance
                    best_feature_index = feature_index
                    best_threshold = threshold
        return {"feature_index": best_feature_index, "threshold": best_threshold, "variance": best_variance}

    def _weighted_variance(self, left_y, right_y):
        weighted_var = (np.var(left_y) * len(left_y) + np.var(right_y) * len(right_y)) / (len(left_y) + len(right_y))
        return weighted_var

dt_model = DecisionTreeRegressor(max_depth=5)
dt_model.fit(x_train_regression, y_train_regression)
dt_predictions = dt_model.predict(x_test_regression)

# Calculating RMSE
rmse = np.sqrt(mean_squared_error(y_test_regression, dt_predictions))
print(f"Decision Tree Regression RMSE: {rmse}")

# Random Forest for Regression

from sklearn.ensemble import RandomForestRegressor
'''
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_reg, y_train_reg)
rf_predictions = rf_model.predict(X_test_reg)
print(f"Random Forest Regression RMSE: {np.sqrt(mean_squared_error(y_test_reg, rf_predictions))}")
'''

class RandomForestRegressor:
    def __init__(self, n_estimators=100, max_depth=None):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_estimators):
            indices = np.random.choice(len(y), len(y), replace=True)
            tree = DecisionTreeRegressor(max_depth=self.max_depth)
            tree.tree = tree.fit(X[indices], y[indices])
            self.trees.append(tree)

    def predict(self, X):
        tree_preds = np.array([tree.predict(X) for tree in self.trees])
        return np.mean(tree_preds, axis=0)

# Training and predictions
rf_model = RandomForestRegressor(n_estimators=100, max_depth=10)
rf_model.fit(x_train_regression, y_train_regression)
rf_predictions = rf_model.predict(x_test_regression)

# Calculate RMSE
from sklearn.metrics import mean_squared_error
rmse = np.sqrt(mean_squared_error(y_test_regression, rf_predictions))
print(f"Random Forest Regression RMSE: {rmse}")

# Random Forest for Regression

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
'''
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_reg, y_train_reg)
rf_predictions = rf_model.predict(X_test_reg)
print(f"Random Forest Regression RMSE: {np.sqrt(mean_squared_error(y_test_reg, rf_predictions))}")
'''

class RandomForestRegressor:
    def __init__(self, n_estimators=100, max_depth=None):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_estimators):
            indices = np.random.choice(len(y), len(y), replace=True)
            tree = DecisionTreeRegressor(max_depth=self.max_depth)
            tree.fit(X[indices], y[indices])  # Fit the tree directly without assigning to tree.tree
            self.trees.append(tree)

    def predict(self, X):
        tree_preds = np.array([tree.predict(X) for tree in self.trees])
        # Handle None values in predictions
        tree_preds = np.where(np.isnan(tree_preds), 0, tree_preds)  # Replace NaNs with 0
        return np.mean(tree_preds, axis=0)

# Training and predictions
rf_model = RandomForestRegressor(n_estimators=100, max_depth=10)
rf_model.fit(x_train_regression, y_train_regression)
rf_predictions = rf_model.predict(x_test_regression)

# Calculate RMSE
from sklearn.metrics import mean_squared_error
rmse = np.sqrt(mean_squared_error(y_test_regression, rf_predictions))
print(f"Random Forest Regression RMSE: {rmse}")